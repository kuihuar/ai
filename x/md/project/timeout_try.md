- Q：下游响应时间变长对我们服务有什么影响
- A： 当请求下游迟迟不能返回结果时，我们服务与下游服务之间的连接就无法释放，而且正在等待请求返回的协程也会被读请求给阻塞住。一旦响应时间变长的请求数量变多，极有可能使我们服务的机器资源被耗尽，最终使得我们的服务崩溃
- 1. 答案就是采取超时主动快速失败
    - Q: 超时时间设置多少比较合适呢
    - A: 可以基于调用下游服务的 p99 延时（99% 的请求都在这个时间内返回），外加一定的冗余时间作为超时时间。而且为了尽量避免无意义的等待，这个超时时间应该小于上游调用我们服务设置的超时时间。
- 2. 超时重试    
    - Q: 超时重试的次数和间隔时间如何设置呢
    - A: 一般设置成 2-3 次，而且这个重试次数，需要小于上游超时时间除以我们调下游的超时时间，避免无效重试, 
    - A: 重试间隔时间一般设置成 100ms 左右，避免无效重试
- 3. 如何防止重试风暴问题
    - Q: 如何防止重试风暴问题
    - A: 你可以采用链路中止策略，对于上游链路过来的重试请求，不再对下游进行超时重试，避免重试风暴问题。而且，对下游的重试调用，你可以设置重试阈值熔断，当在一个时间窗口内，重试请求达到正常请求的一定比例，就不再进行重试，这样就能避免大面积重试把下游打垮    


#### 总结
1. 首先，我们需要对下游调用设置合理的超时时间，避免因长时间等待下游返回，我们服务的机器资源不能释放而耗尽。
2. 接下来，如果对下游的调用是重要的请求类型，比如说涉及到关键业务流程的校验等环节，在保证下游调用是幂等的情况下，我们需要进行超时重试，尽量提升我们服务整体的可用性。
3. 再然后，我们需要采用链路中止策略，避免重试风暴给下游造成较大压力。
4. 最后，我们需要设置重试阈值熔断，控制重试比例，避免大面积超时重试直接把下游打崩。    
5. kitex微服务框架重试的策略选择(3.1.1.2)部分https://www.cloudwego.io/zh/docs/kitex/tutorials/service-governance/retry/#1-%E9%87%8D%E8%AF%95%E5%8A%9F%E8%83%BD%E8%AF%B4%E6%98%8E




### 熔断：下游服务过载怎么办？
熔断机制指的是当调用某个服务出现超时等指定的错误，并且在一段时间内的错误率超过了预先设定的阈值时，我们的服务就自动停止向下游服务发送后续请求，转而暂时返回错误信息或者备用数据，以保护我们的服务不被大量的请求拖垮。

#### 熔断器， 实现了熔断机制的组件，我们一般称为熔断器（circurt breaker）
- 首先是闭合状态（Closed）。当处于这种状态时，每次对下游服务发起调用，系统都会记录指定错误的失败次数以及总调用次数。在一定的时间窗口内，一旦失败次数达到预设阈值，就会触发熔断，状态转变为断开状态。
- 接着是断开状态（Open）。在该状态下，针对下游的请求将不再调用后端服务，而是直接立即返回错误响应。与此同时，会启动一个计时器，当定时器计时结束，便会进入半熔断状态。
- 最后是半熔断状态 (Half-Open)。在这一状态下，系统允许向故障服务发送少量试探性请求，这是为了检测服务是否已恢复正常。如果这些试探性调用都能正常完成，就可判定被调用服务已恢复，这时熔断器会切回闭合状态，同时重置相关计数。倘若这些试探性调用中仍存在失败的情况，那就表明被调用服务尚未恢复，熔断器会重新切换到断开状态，并重置计时器。半熔断状态能有效避免正在恢复的服务，因突然大量涌入的请求而再次被打垮。
#### 熔断粒度
- 服务粒度熔断，按照服务粒度进行熔断统计
- 接口粒度熔断，按照方法级别的异常做熔断统计
- 实例粒度熔断，按照实例粒度进行熔断统计
- 同时启用接口粒度和实例粒度的熔断
- 熔断机制的主要作用是缓解系统过载，因此我们需要准确识别与系统过载相关的错误类型，以便进行有效的熔断处理。常见的过载相关错误包括超时和限流等
- 在实际应用中，微服务框架一般都会集成熔断器，我们无需自行开发。如果我们使用的微服务框架没有集成熔断器，也有不少开源的第三方组件可供选择。例如，Netflix 开源的 hystrix-go，以及阿里开源的 sentinel-golang，都能为我们提供熔断功能支持。


### 降级：如何保证核心功能和场景可用？
当下游服务在短时间内无法恢复正常运行时，仅仅依靠熔断机制是不够的。这时我们需要从业务功能层面入手，搭配降级的方案来实现快速止损
降级是指服务本身资源不足或下游服务不可用时，通过提供默认结果或关闭非核心功能等手段，来保证系统的核心功能能够继续运行，从而提高系统的可用性和稳定性。

降级方案的核心在于精心设计降级策略。由于业务场景的多样性，相应的降级策略也会各不相同

- 第一种策略是返回兜底数据。
- 第二种策略是异步处理。
- 第三种策略是关闭非核心功能。
- 第四种策略是限流。
- 第五种策略是熔断。

关于降级方式的选择，我们有两种主要途径：一是利用远程配置开关来实现灵活的降级控制，二是依赖自动降级机制以实现快速响应

### 服务限流
限制服务在单位时间内处理的请求数量，当请求量超过预先设定的阈值时，就直接拒绝超出部分的请求

#### 限流算法
- 固定窗口算法
    - 抗抖动能力较弱
    - 时间窗口边界限流不准
- 滑动窗口算法
    - 时间窗口边界限流准确
    - 存在突刺现象
- 漏桶算法
    - 平滑流量，避免突刺现象
    - 存在流量损失
- 令牌桶算法
    - 平滑流量，避免突刺现象
    - 存在流量损失
    - 支持突发流量        
#### 适用场景
1. 以固定窗口计数器算法为例，它在流量较为均匀且对限流精度要求不高的情况下，能够发挥重要作用。例如，在一些日常访问量相对稳定、无需精细控制请求频率的系统中，固定窗口算法可以简单有效地控制请求量
2. 而漏桶算法则特别适用于对接第三方 API 的场景。当第三方 API 对调用频率有明确限制时，为了防止因超出频率限制而导致调用失败，我们需要在客户端进行精准限流。此时，漏桶算法无疑是最佳选择。
#### 限流对象
- 首先是上游服务唯一标识
- 接着是本服务接口名称
- 然后是本服务集群名称
- 最后是上游服务集群名称
#### 限流方式

1. 集中式限流
2. 单机本地限流
3. 分布式限流
4. 混合式限流

- 集中式限流是在系统架构中借助一个中心化的组件（如 Redis）来集中统一地管理和执行限流逻辑
- 本地限流是指在单个服务器或服务实例上进行限流，通过限制单台服务器在单位时间内处理的请求数量，防止服务器过载
- 在微服务架构体系下，服务调用通常会借助负载均衡机制来实现。基于此，我们可以合理假设各个单机所承担的流量大致相当。因此，在进行服务限流时，本地限流便成为了首选方案。具体操作中，我们会将整体的限流阈值均匀分配到每台机器上，以此作为单机本地限流的精准阈值，从而确保整个服务在高并发场景下的稳定运行与高效处理。
```shell
https://pkg.go.dev/golang.org/x/time/rate
https://github.com/uber-go/ratelimit/
```

#### 故障隔离
在分布式系统架构中，为了防止某些模块或组件出现异常故障时波及到其他正常运行的模块，我们也会预先实施故障隔离措施。具体来说，我们会采用特定的方法在不同模块之间设置隔离机制，确保一旦某一模块发生故障，它不会干扰到其他模块正常运作
##### 隔离设计策略
- 数据隔离
    - 行级别的隔离
    - 表级别的隔离
    - 库级别的隔离
- 集群隔离(计算层级隔离)
    - 离线
    - 在线
    - 核心业务
    - 非核心业务
- 服务拆分
    - 核心业务拆分为独立的业务
    - 非核心业务拆分为共享的业务（共享资源）


### 部署策略
实际操作过程中，很多公司会采用灰度部署和滚动部署相结合的发布策略
#### 停机部署
#### 蓝绿部署
#### 滚动部署
#### 灰度部署
- 第一种是基于用户 ID 的灰度方案
- 第二种是基于用户角色的灰度方案
- 第三种是基于地理位置的灰度
- 第四种是基于用户行为的灰度
- 第五种是基于设备类型的灰度 

json序列化sonic
字节开源的高性能网络库netpoll为例
